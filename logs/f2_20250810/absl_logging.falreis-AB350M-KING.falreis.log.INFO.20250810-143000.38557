I0810 14:30:02.477980 137061998142976 xla_bridge.py:889] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0810 14:30:02.478631 137061998142976 xla_bridge.py:889] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0810 14:30:02.714267 137061998142976 run.py:410] Model: f2 ['task_scheduling']
I0810 14:30:02.714373 137061998142976 run.py:412] algorithms ['task_scheduling']
I0810 14:30:02.714612 137061998142976 run.py:413] train_lengths [-1]
I0810 14:30:02.714661 137061998142976 run.py:414] train_batch_size 16
I0810 14:30:02.714789 137061998142976 run.py:415] val_batch_size 8
I0810 14:30:02.714843 137061998142976 run.py:416] test_batch_size 8
I0810 14:30:02.714883 137061998142976 run.py:417] chunked_training True
I0810 14:30:02.715046 137061998142976 run.py:418] chunk_length 16
I0810 14:30:02.715091 137061998142976 run.py:419] train_steps 10000
I0810 14:30:02.715131 137061998142976 run.py:420] eval_every 50
I0810 14:30:02.715171 137061998142976 run.py:421] test_every 500
I0810 14:30:02.715214 137061998142976 run.py:422] learning_rate 0.001
I0810 14:30:02.715346 137061998142976 run.py:423] grad_clip_max_norm 1.0
I0810 14:30:02.715387 137061998142976 run.py:424] dropout_prob 0.1
I0810 14:30:02.715427 137061998142976 run.py:425] hint_teacher_forcing 0.0
I0810 14:30:02.715465 137061998142976 run.py:426] hint_mode encoded_decoded
I0810 14:30:02.715618 137061998142976 run.py:427] hint_repred_mode hard_on_eval
I0810 14:30:02.715662 137061998142976 run.py:428] use_ln False
I0810 14:30:02.715701 137061998142976 run.py:429] use_lstm True
I0810 14:30:02.715738 137061998142976 run.py:430] nb_triplet_fts 8
I0810 14:30:02.715776 137061998142976 run.py:431] encoder_init xavier_on_scalars
I0810 14:30:02.715817 137061998142976 run.py:432] processor_type f2
I0810 14:30:02.715855 137061998142976 run.py:433] checkpoint_path CLRS30
I0810 14:30:02.715892 137061998142976 run.py:434] dataset_path CLRS30
I0810 14:30:02.715934 137061998142976 run.py:435] freeze_processor False
I0810 14:30:02.715973 137061998142976 run.py:436] reduction max
I0810 14:30:02.716011 137061998142976 run.py:437] activation elu
I0810 14:30:02.716047 137061998142976 run.py:438] restore_model 
I0810 14:30:02.716088 137061998142976 run.py:439] gated True
I0810 14:30:02.716126 137061998142976 run.py:440] gated_activation sigmoid
I0810 14:30:02.719461 137061998142976 run.py:466] Creating samplers for algo task_scheduling
I0810 14:30:02.719677 137061998142976 run.py:254] Dataset found at CLRS30/CLRS30_v1.0.0. Skipping download.
I0810 14:30:02.720480 137061998142976 dataset_info.py:708] Load dataset info from CLRS30/CLRS30_v1.0.0/clrs_dataset/task_scheduling_train/1.0.0
I0810 14:30:02.724355 137061998142976 dataset_info.py:708] Load dataset info from CLRS30/CLRS30_v1.0.0/clrs_dataset/task_scheduling_train/1.0.0
I0810 14:30:02.728553 137061998142976 reader.py:262] Creating a tf.data.Dataset reading 1 files located in folders: CLRS30/CLRS30_v1.0.0/clrs_dataset/task_scheduling_train/1.0.0.
I0810 14:30:02.786161 137061998142976 logging_logger.py:49] Constructing tf.data.Dataset clrs_dataset for split train, from CLRS30/CLRS30_v1.0.0/clrs_dataset/task_scheduling_train/1.0.0
W0810 14:30:02.806743 137061998142976 ag_logging.py:142] AutoGraph could not transform <function _preprocess at 0x7ca7b72c6d40> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: `haiku.experimental.flax` features require `flax` to be installed.
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
I0810 14:30:02.882519 137061998142976 run.py:254] Dataset found at CLRS30/CLRS30_v1.0.0. Skipping download.
I0810 14:30:02.883140 137061998142976 dataset_info.py:708] Load dataset info from CLRS30/CLRS30_v1.0.0/clrs_dataset/task_scheduling_val/1.0.0
I0810 14:30:02.885764 137061998142976 dataset_info.py:708] Load dataset info from CLRS30/CLRS30_v1.0.0/clrs_dataset/task_scheduling_val/1.0.0
I0810 14:30:02.887968 137061998142976 reader.py:262] Creating a tf.data.Dataset reading 1 files located in folders: CLRS30/CLRS30_v1.0.0/clrs_dataset/task_scheduling_val/1.0.0.
I0810 14:30:02.920824 137061998142976 logging_logger.py:49] Constructing tf.data.Dataset clrs_dataset for split val, from CLRS30/CLRS30_v1.0.0/clrs_dataset/task_scheduling_val/1.0.0
I0810 14:30:02.997875 137061998142976 run.py:254] Dataset found at CLRS30/CLRS30_v1.0.0. Skipping download.
I0810 14:30:02.998447 137061998142976 dataset_info.py:708] Load dataset info from CLRS30/CLRS30_v1.0.0/clrs_dataset/task_scheduling_test/1.0.0
I0810 14:30:03.000983 137061998142976 dataset_info.py:708] Load dataset info from CLRS30/CLRS30_v1.0.0/clrs_dataset/task_scheduling_test/1.0.0
I0810 14:30:03.002923 137061998142976 reader.py:262] Creating a tf.data.Dataset reading 1 files located in folders: CLRS30/CLRS30_v1.0.0/clrs_dataset/task_scheduling_test/1.0.0.
I0810 14:30:03.035790 137061998142976 logging_logger.py:49] Constructing tf.data.Dataset clrs_dataset for split test, from CLRS30/CLRS30_v1.0.0/clrs_dataset/task_scheduling_test/1.0.0
